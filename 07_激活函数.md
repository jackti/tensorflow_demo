## (1)激活函数

TensorFlow提供的激活函数有：

- `tf.sigmoid(x,name=None)` 表达式为$sigmoid(x)=1/(1+\exp(-x))$

- `tf.tanh(x,name=None)`表达式为$tanh(x) = \dfrac{e^x - e^{-x}}{e^x + e^{-x}}$

- `tf.nn.relu(features,name=None)`表达式为$relu(x)=max(x, 0)$

- `tf.nn.relu6(features,name=None)` 表达式为$relu6(x)=min(max(0,x),6)$

- `tf.nn.softplus(features,name=None)`表达式为$softplus(x)=log(exp(x)+1)$

- `tf.nn.dropout(x,keep_prob,nosie_shape=None,seed=None,name=None)`

  使输入tensor中某些元素变为0，其它没变0的元素值变为原来的1/keep_prob大小。输入和输出的shape相同。

- `tf.nn.bias_add(value,bias,name=None)`

  对value加一偏置量 此函数为tf.add的特殊情况，bias仅为一维， 函数通过广播机制进行与value求和, 数据格式可以与value不同，返回为与value相同格式



## (2)数据标准化

`tf.nn.l2_normalize(x,dim,epsilon=1e-12,name=None)`

对维度dim进行L2范式标准化，输出结果为$y=x/sqrt(max(sum(x^2),epsilon))$ 



`tf.nn.moments(x,axes,shift=None,name=None,keep_dims=False)`

直接计算均值和方差



`tf.nn.sufficient_statistics(x,axes,shift=None,keep_dims=False,name=None)`

计算与均值和方差有关的完全统计量，返回4维数组

`tf.nn.normalize_moments(counts, mean_ss, variance_ss, shift, name=None)`

基于完全统计量计算均值和方差





## (3)损失函数

`tf.nn`模块是比较底层的封装，`tf.losses`有更加高级的封装。

####回归函数

`tf.nn.l2_loss(t,name=None)`

通常在回归问题的优化目标中加入参数的正则项，防止过拟合。表达式为$\frac{1}{2}\sum{w^2}$

#### 分类函数

`tf.nn.softmax(logits,name=None)`

`tf.nn.log_softmax(logits,name=None)`

上面两个函数分别计算softmax值和log_softmax值

`tf.nn.sigmoid_cross_entropy_with_logits(logits,target,name=None)`

`tf.nn.softmax_cross_entropy_with_logits(logits,labels,name=None)`

`tf.nn.sparse_softmax_cross_entropy_with_logits (logits, labels, name=None)`

`tf.nn.weighted_cross_entropy_with_logits (logits, targets, pos_weight, name=None)`

计算logits和labels之间的交叉熵。



下面详细对比说明下三个函数的区别对比：

①`tf.nn.softmax_cross_entropy_with_logits`

②`tf.nn.sparse_softmax_cross_entropy_with_logits`

③`tf.nn.softmax_cross_entropy_with_logits_v2`

其中①和②对于参数`logits`的要求都是一样的，即未经处理直接由神经网络输出的结果数值，如[3.5,2.1,7.8,4.3]两者的区别在于对于labels格式要求不一样：①函数要求labels的格式和logits类型，如[0,0,1,0]；但是②要求labels是一个数值，这个数值记录了样本所属的类别，比如[0,0,1,0]为例，②的要求labels的输入为数字2。一般可以使用`tf.argmax()`来从[0,0,1,0]中取得类别数值。

①和③是相似的，但是官方文档推荐使用③，标记①为deprecated状态。两者的区别在于①在进行反向传播的时候，只对logits进行反向传播，labels保持不变。而③在进行反向传播的时候，会同时对logits和labels都进行反向传播（如果对③的labels设置为stop_gradients,那么就和①的功能相同了）。

一般在进行监督学习的时候，labels都是标记好的真值，为什么会需要改变label呢？③存在的意义是什么？实际上在应用中labels并不一定都是人工标注的，有时候还有可能是神经网络生成的，比如对抗生成网络GAN。





















